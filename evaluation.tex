\chapter{Evaluation}
\label{c:evalu}

In this chapter, we present the end-to-end evaluation of our system.
At first, we discuss the evaluation datasets and ground truth annotation.
Next, we present the quantitative results for both computation time and accuracy.
Finally, we conclude this chapter with evaluation of our system using a public dataset.

\section{Evaluation datasets}
\label{s:eval}
To our best knowledge, there is not much research on sensor fusion in the context of traffic light detection, especially for pedestrian navigation. 
As a result, we did not find public datasets combining traffic lights video and inertial sensor data. 
Hence, we collected our own ground truth data using an Android app.
% \todo{refer section or fig with the screenshot}.
We walked across several street crossings and recorded both video and sensor data simultaneously in various lighting conditions. 
For example, \ref{f:dataset} shows video frames for both sunny and cloudy days. 
Here, we present our results for several 300 feet long walks.  
At the end of this chapter, we present an approximate evaluation of a public dataset that does not have sensor data. 
However, we emulate the effect of sensors by manual selection of a subpart of a video frame. 

\begin{figure}[!ht]
\centering
\subfloat[Sunny] {\includegraphics[width=3.1in]{images/sunny.jpg}}
\hfill
\subfloat[Cloudy] {\includegraphics[width=3.1in]{images/cloudy.jpg}}
\caption{Scene variation of recorded video.}
\label{f:dataset}
\end{figure}

\ref{t:dataset} shows the total no of frames and time duration of our dataset.

\begin{table}[h!]
  \centering
  \caption{Description of the dataset.}
  \label{t:dataset}
  \rowcolors{2}{gray!25}{white}
  \begin{tabular}{  l  c  r  }
    \rowcolor{gray!50}
    Name & Frame Count & Time Duration \\
    \hline
    Walking w/ sensor (Sunny day) & 5905 & 3 mins 16 secs  \\
    Walking w/ sensor (Cloudy day) & 6205 & 3 mins 26 secs \\
    Walking w/ sensor (Regular day) & 6022 & 3 mins 20 secs \\
    Static w/ sensor (Regular day) & 1810 & 1 min \\
    \hline
  \end{tabular}
\end{table}

\section{Annotation}
We need the ground truth for traffic light positions on video frames in order to measure our system performance quantitatively.
Accordingly, we annotate the traffic light's positions manually by drawing a rectangle around the traffic lights at each video frame.
During annotation, we record the type of the traffic lights (i.e., red or green) and their positions with a bounding box.

\ref{f:annotate} shows the interface for manual annotation.
The green box provides the location of the traffic light and we annotate 0 for a red traffic light and 1 for a green traffic light.

\begin{figure}[h!]
\centering
\includegraphics[width=5.2in]{images/annotation.png}
\caption{Interface for manual annotation.}
\label{f:annotate}
\end{figure}


\section{Computation time}
We collected data at different times of the day as we discussed in \S\ref{s:eval}.
In this section, we discuss the computation time of these datasets with and without the sensor fusion.

\subsection{Frame processing time}
\ref{f:cdf_cloudy} shows the computation time CDF for video frames for walking dataset with sensor movement in cloudy weather described in \ref{t:dataset}.
\begin{figure}[ht]
\centering
\includegraphics[width=5.2in]{plots/cloudy_cdf.pdf}
\caption{CDF of frame computation time for walking dataset with sensor movement in cloudy weather.}
\label{f:cdf_cloudy}
\end{figure}
It shows that the median of the computation time without the sensor data and without the heuristic filters is 108.62ms.
On the other hand, the median computation time with the sensor and without heuristic filters is 13.11ms.
This is an improvement of 8.29x.
The median computation time with sensor and heuristic filters is 18.25ms.
There is a slight increase in frame processing time with the heuristic filters, but this is worthwhile as we false detection rate reduces significantly with the heuristic filters.  
We discuss more about the accuracy and false detections in Section \S\ref{s:acc}

\ref{t:dataset_time} shows the median computation time for other datasets described in \ref{t:dataset}.
It shows that the average median computation time without sensor and without our heuristic filter is 113.11 ms.
On the contrary, the average median computation time with sensor and without our heuristic filter is 13.24 ms.
This is the average improvement is 8.54x.
The average median computation time with sensor and heuristic filter is 19.53 ms.
This gives the average improvement 5.79x, which is trivial decrease in computation time improvement, but this gives less false detection rate as we described earlier.

\begin{table}[!ht]
  \centering
  \caption{Median computation time (ms) with various settings for our dataset.}
  \label{t:dataset_time}
  \rowcolors{2}{gray!25}{white}
  \begin{tabular}{  l r r r r}
    \rowcolor{gray!50}
    Dataset Nane & W/o sensor & w/o sensor & w/ sensor  & w/ sensor \\
    \rowcolor{gray!50}
    & w/o filter & w/ filter & w/o filter & w/ filter\\
    \hline
    Walking w/ sensor (Cloudy) & 108.62 & 109.76 & 13.11 & 18.25 \\
    Walking w/ sensor (Sunny) & 112.74 & 113.67 & 13.32 & 19.82 \\
    Walking w/ sensor (Regular) & 110.43 & 114.12 & 13.89 & 20.17 \\
    Static w/ sesor (Regular) & 120.65 & 121.92 & 12.65 & 19.89\\
    \hline
    Avg. computation time (ms) & 113.11 & 114.87 & 13.24 & 19.53\\
    
  \end{tabular}
\end{table}

\subsection{Subimage processing time}
Processing a subpart of a video frame significantly reduces the computation time. 
We select a Region-Of-Interest (ROI) area within a frame with the sensor hints.
However, the ROI predicted from the sensor hints can be incorrect and we gradually increase the area of the rectangle.
We discussed the details about this in \S\ref{s:roi}.


\ref{f:recarea} shows the computation time with the increase of the ROI area in video frames.
It shows that the computation time increases as the area of the rectangle get larger.
For the same area, if the number of candidate pixels is high or the detected circle count is high then computation time can be also high.

\begin{figure}[h!]
\centering
\includegraphics[width=5.2in]{plots/cloudy_recarea.pdf}
\caption{Computation time with the increase of the rectangle area.}
\label{f:recarea}
\end{figure}



\subsection{Time for heuristic filtering}
We use a heuristic filter to reduce false positive in traffic light detection as we discuss at \S\ref{s:filter}.
The computational cost of the heuristic filter is very small.

\begin{figure}[h!]
\centering
\includegraphics[width=5.2in]{plots/sunny_cdf_filter.pdf}
\caption{CDF of computation time for the heuristic filter.}
\label{f:cdf_fil}
\end{figure}

\ref{f:cdf_fil} shows the computation time of the heuristic filter that we discuss at \S\ref{s:filter}.
The computation time depends on the number of circles detected on the frame.
If circle count is high, filtering need for all of these circles, so computation time gets higher.
\ref{f:cdf_fil} shows that the median computation time is 1.5 ms for the filtering. 





\section{Traffic lights detection accuracy}
\label{s:acc}
To demonstrate the robustness of the various traffic light scenarios, we recorded video at different lightening condition such as cloudy and sunny and at the different time of the day.
We walked along several crosswalks of few streets and the route had a total of 16 traffic lights.

\subsection{Confusion matrix}
\ref{t:con_nocrp} shows the confusion matrix for the traffic light decision when we do not consider the sensor hints of the smartphone.
\ref{t:con_crp} shows the confusion matrix considering the sensor hints in the same dataset.

\begin{table}[h!]
  \centering
  \caption{Confusion Matrix without sensor hints for static movement dataset. Entries are number of decisions. Each row and column has an associated accuracy.}
  \label{t:con_nocrp}
  \begin{tabular}{  l | c | c | r }
   
     & Detected Red & Detected Green &  \\
    \hline
    Actual Red & 858 & 52 & 94.29\% \\
    \hline
    Actual Green & 51 & 509 & 90.89\% \\
    \hline
    & 94.39\% & 90.73\% & 92.99\% \\
    
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Confusion Matrix with sensor hints for static movement dataset.Entries are number of decisions. Each row and column has an associated accuracy.}
  \label{t:con_crp}
  \begin{tabular}{  l | c | c | r }
   
     & Detected Red & Detected Green &  \\
    \hline
    Actual Red & 909 & 1 & 99.89\% \\
    \hline
    Actual Green & 36 & 524 & 93.57\% \\
    \hline
    & 96.19\% & 99.81\% & 97.48\% \\
    
  \end{tabular}
\end{table}

These results show that the use of sensor hints increases the accuracy of the red light detection and reduces false detection of green lights.


\subsection{Detection and misdetection rate for traffic lights}
\ref{f:tp_stat} shows the detection rate for the red and green state of the traffic lights.
It shows that using the sensor hints detection rate for red lights increases from 86\% to 96\% and the detection rate for green lights increases 96\% to 99\%.

\begin{figure}[h!]
\centering
\includegraphics[width=5.2in]{plots/bar_tp.pdf}
\caption{Detection rate for static movement dataset.}
\label{f:tp_stat}
\end{figure}

\ref{f:fp_stat} shows the misdetection rate for the red and green state of traffic lights.
Left one is the false positive detection and the right one is the false negative detection for the traffic light detection.
Here, the false positive count is reduced significantly and false negative is zero.

\begin{figure}[!ht]
\centering
\subfloat[] {\includegraphics[width=5.2in]{plots/bar_fp.pdf}}

\subfloat[] {\includegraphics[width=5.2in]{plots/bar_fn.pdf}}
\caption{Misdetection rate for static movement dataset.}
\label{f:fp_stat}
\end{figure}

\ref{t:acc_stat} shows the accuracy rate for the static with sensor movement dataset.
It shows that the accuracy increases from 89\% to 97\% with sensor hints.

\begin{table}[h!]
  \centering
  \caption{Accuracy for detection in our datasets.}
  \rowcolors{2}{gray!25}{white}
  \label{t:acc_stat}
  \begin{tabular}{  l  r  r r r }
    \rowcolor{gray!50}
    Dataset & w/o sensor & w/o sensor & w/ sensor & w/ sensor \\
    \rowcolor{gray!50}
    name & w/o filter & w/ filter & w/o filter & w/ filter \\
    \hline
    Static w/ sensor (Regular) & 89.7313\% & 90.6142\% & 97.035\% & 98.7908\% \\
    \hline
  \end{tabular}
\end{table}


\section{Evaluation of a public dataset}

In this section, we evaluate our system with a well studied public dataset, LISA Traffic Light Dataset \cite{lisa}.
The LISA Traffic Light Dataset consists of 13 day training clips and 5 night traing clips with 4 testing sequences which are captured in San Diego, California, USA \cite{lisa2}.
This dataset has total 46418 frames with 112,971 annotated lights.

The main approach of our system is to use sensor hints to improve the comutational time and the detection accuracy.
However, the LISA dataset has no information of the sensor hints, so we manually take the 1/4 frames.

\ref{f:lisa_cdf} shows the CDF of computation time for dayclip1 dataset with full frames and subframes.
It shows that the median time for full frame is 67.15 ms and for the subframe is 12.03 ms.
We can improve the computation time by 5.58 for this dataset taking the approximate 1/4 of frames.
However, if we have sensor hints we can improve computation time more keeping the subframes area smaller.

\ref{t:lisa_time} shows the median computation time for full frames and subframes for other clips of LISA dataset.
It shows that the average median computation time for full frame is 59.29 ms and for subframe is 12.53 ms.


\begin{figure}[ht!]
  \centering
  \includegraphics[width=5.2in]{plots/lisaday1_cdf.pdf}
  \caption{CDF time of dayClip1 dataset with cropping and without cropping.}
  \label{f:lisa_cdf}
\end{figure}

\begin{table}[!ht]
  \centering
  \caption{Median computation time (ms) for LISA dataset.}
  \label{t:lisa_time}
  \rowcolors{2}{gray!25}{white}
  \begin{tabular}{  l   r   r }
    \rowcolor{gray!50}
    Sequence Nane &  Full frame (ms)  &  Subframe (ms) \\
    \hline
    Dayclip-1 & 67.15 & 12.03 \\
    Dayclip-2 & 53.85 & 10.10 \\
    Dayclip-3 & 58.63 & 10.81 \\
    Dayclip-4 & 51.89 & 9.67 \\
    Dayclip-5 & 68.23 & 13.05 \\
    Dayclip-6 & 52.24 & 10.01 \\
    Dayclip-7 & 68.11  & 13.20 \\
    Dayclip-8 & 52.16  & 9.89  \\
    Dayclip-9 & 54.22  & 10.40  \\
    Dayclip-10 & 58.66 & 10.80 \\
    Dayclip-11 & 53.95 & 10.40 \\
    Dayclip-12 & 62.50  & 13.05 \\
    Dayclip-13 & 54.33 & 10.09 \\
    Nightclip-1 & 63.19 & 19.10 \\
    Nightclip-2 & 66.48 & 18.55 \\
    Nightclip-3 & 62.30 & 16.50 \\
    Nightclip-4 & 60.43 & 13.45 \\
    Nightclip-5 & 58.89 & 14.50 \\
    \hline
    Average computation time (ms) & 59.29 & 12.53 \\
    
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Confusion Matrix without cropping for LISA dataset of dayclip1.}
  \label{t:con_nocrp}
  \begin{tabular}{  l | c | c | r }
   
     & Detected Red & Detected Green &  \\
    \hline
    Actual Red & 324 & 23 & 93.37\% \\
    \hline
    Actual Green & 58 & 111 & 65.68\% \\
    \hline
    & 84.82\% & 82.84\% & 84.30\% \\
    
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Confusion Matrix with cropping for LISA dataset of dayclip1.}
  \label{t:con_crp}
  \begin{tabular}{  l | c | c | r }
   
     & Detected Red & Detected Green &  \\
    \hline
    Actual Red & 342 & 5 & 98.55\% \\
    \hline
    Actual Green & 14 & 155 & 91.72\% \\
    \hline
    & 96.07\% & 96.88\% & 96.32\% \\
    
  \end{tabular}
\end{table}
11
Red 24.4514 0
Green 0 0
Red 70.5329 80.8464
Green 67.5439 87.6537
Red 13.4796 9.1536
Green 20.1754 4.4561
Acc 69.5853 81.8157

12
Green fp 10 8.33333
Green tp 78.3333 96.6667
Green fn 17.5 1.66667
Acc 77.686 95.8678


\section{Effect of video/image resolution}

\todo{\ref{f:vf_res} shows the computation time after changing video frames resolution.}

\begin{figure}[h!]
  \centering
  \vspace{2in}
  %\includegraphics[width=5.2in]{plots/bar_tp.pdf}
  \caption{Effect of the computation time after changing video frames resolution.}
  \label{f:vf_res}
\end{figure}

